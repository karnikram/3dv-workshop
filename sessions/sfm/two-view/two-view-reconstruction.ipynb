{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-view reconstruction\n",
    "\n",
    "A toy exercise to reconstruct the sparse 3D geometry of a scene from two arbitrary (non-stereo) images.\n",
    "\n",
    "![sample-two-view.png](sample-two-view.png)\n",
    "\n",
    "Concepts used:\n",
    "- Feature extraction and matching\n",
    "- Essential matrix\n",
    "- Linear triangulation\n",
    "\n",
    "@author: Karnik (karnikram@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def load_image(path):\n",
    "    \"\"\" Loads image from the given path and returns\n",
    "    it in grayscale format.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        print(path + ' could not be loaded!')\n",
    "        \n",
    "    return img\n",
    "\n",
    "def extract_features(img):\n",
    "    \"\"\" Extracts keypoints from the image and computes their descriptors.\n",
    "    \n",
    "    We use OpenCV's implementation of SIFT to do this.\n",
    "    \n",
    "    Args:\n",
    "        img: The input grayscale image.\n",
    "    \n",
    "    Returns:\n",
    "        kp: The detected keypoints.\n",
    "        ds: The computed feature descriptors needed  for matching.      \n",
    "    \"\"\"\n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures = 2000)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    \n",
    "    return kp, des\n",
    "\n",
    "def match_features(kp1, kp2, des1, des2):\n",
    "    \"\"\" Matches keypoints from two images using their descriptors.\n",
    "    \n",
    "    We use OpenCV's implementation of brute-force matching to do this.\n",
    "    \n",
    "    Args:\n",
    "        kp1: Detected keypoints from the first image.\n",
    "        kp2: Detected keypoints from the second image.\n",
    "        des1: Descriptors of the keypoints from the first image.\n",
    "        des2: Descriptors of the keypoints from the second image.\n",
    "        \n",
    "    Returns:\n",
    "        matches: List of DMatch objects needed for visualization.\n",
    "        pts1: The 2d image coordinates of matched keypoints from first image.\n",
    "        pts2: The 2d image coordinates of matched keypoints from second image.\n",
    "    \"\"\"\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    \n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "\n",
    "    for match in matches:\n",
    "        pts1.append(kp1[match.queryIdx].pt)\n",
    "        pts2.append(kp2[match.trainIdx].pt)\n",
    "        \n",
    "    pts1 = np.array(pts1, dtype=np.float32)\n",
    "    pts2 = np.array(pts2, dtype=np.float32)\n",
    "    \n",
    "    return matches, pts1, pts2\n",
    "\n",
    "def find_essentialmat(K, pts1, pts2):\n",
    "    \"\"\" Finds the essential matrix between two views given matching points.\n",
    "    \n",
    "    We use OpenCV's implementation of the five-point algorithm for estimating the matrix.\n",
    "    The estimation is wrapped inside a RANSAC scheme for robustness against incorrect matches.\n",
    "    \n",
    "        Args:\n",
    "            K: Camera calibration matrix.\n",
    "            pts1: nx2 image points from first image.\n",
    "            pts2: nx2 matching image points from second image.\n",
    "            \n",
    "        Returns:\n",
    "            E: The essential matrix.\n",
    "            pts1: nx2 _inlier_ image points from first image.\n",
    "            pts2: nx2 _inlier_ image points from second image.\n",
    "    \"\"\"\n",
    "    \n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, threshold=1)\n",
    "\n",
    "    mask = mask[:,0]\n",
    "    m_pts1 = []\n",
    "    m_pts2 = []\n",
    "    for idx, pt in enumerate(pts1):\n",
    "        if(mask[idx]):\n",
    "            m_pts1.append(pts1[idx,:])\n",
    "            m_pts2.append(pts2[idx,:])\n",
    "\n",
    "    pts1 = np.array(m_pts1)\n",
    "    pts2 = np.array(m_pts2)\n",
    "    \n",
    "    return E, pts1, pts2\n",
    "\n",
    "def make_homogeneous_pose(R, t):\n",
    "    \"\"\" Constructs a 4x4 homogeneous transformation matrix from a 3x3 rotation matrix\n",
    "    and a 3x1 translation vector.\n",
    "    \"\"\"\n",
    "    return np.concatenate((np.concatenate((R,t),axis=1),np.array([[0,0,0,1]])),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "We've provided you two images of a scene that we want to reconstruct.\n",
    "These two images, however, need to be from a calibrated camera i.e. a camera whose internal parameters have already been estimated. (**Q**: What are these internal parameters?)\n",
    "\n",
    "Recall that to estimate these parameters we use multiple images of a known pattern. Images of one such pattern taken from the same camera have been provided in `../calibration/`. Run the provided `../calibration/calib.py` script on these images to estimate this camera's internal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TODO #########\n",
    "K = np.array([[?, ?, ?],\n",
    "              [?, ?, ?],\n",
    "              [?, ?, ?]])\n",
    "\n",
    "\n",
    "img1 = load_image('./img1.png')\n",
    "img2 = load_image('./img2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "Next, we identify a few salient points (or 'keypoints') from the images that will help us to estimate the camera motion between the two images. Here, we extract these keypoints using the `extract_features` function and then we visualize them. Notice that almost no keypoints belong to the table surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, des1 = extract_features(img1)\n",
    "kp2, des2 = extract_features(img2)\n",
    "\n",
    "fix, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img1, cmap='gray')\n",
    "ax[1].imshow(img2, cmap='gray')\n",
    "\n",
    "for idx in range(len(kp1)):\n",
    "    ax[0].plot(kp1[idx].pt[0], kp1[idx].pt[1], 'r*')\n",
    "    \n",
    "for idx in range(len(kp2)):\n",
    "    ax[1].plot(kp2[idx].pt[0], kp2[idx].pt[1], 'b*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature matching\n",
    "\n",
    "We now wish to match these extracted keypoints from both the images. That is, we want to identify keypoints from both the images that correspond to the same 3D point in the scene. Here, we do this by matching the feature descriptor vectors using the `match_features` function and then we visualize the matches. Notice that there are some incorrect matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches, pts1, pts2 = match_features(kp1, kp2, des1, des2)\n",
    "draw_params = dict(matchColor = (0,255,0), singlePointColor=(255,0,0), flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, **draw_params)\n",
    "\n",
    "plt.imshow(img3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating camera motion\n",
    "\n",
    "Next, we estimate the camera motion between the two frames by estimating the corresponding essential matrix. Recall that the essential matrix is a 3x3 matrix that encodes the relative orientation between two calibrated views.\n",
    "\n",
    "![epipolar-geometry.png](epipolar-geometry.png)\n",
    "\n",
    "[Img. courtesy: Kitani]\n",
    "\n",
    "**Q** What is the difference between the essential matrix and a homography matrix?\n",
    "\n",
    "Also recall that $\\mathrm{E} = [\\mathrm{t}]_\\times \\mathrm{R}$, where $\\mathrm{t}$ and $\\mathrm{R}$ are the relative translation and rotation between the two views respectively. Hence we can estimate the relative transformation between the two views by first estimating their associated essential matrix, and then decomposing the matrix into its constituent rotation and translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, pts1, pts2 = find_essentialmat(K, pts1, pts2)\n",
    "_, R, t, _ = cv2.recoverPose(E, pts1, pts2, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we obtain the relative transformation, we construct the camera matrices $\\mathrm{P_1}$ and $\\mathrm{P_2}$ as $\\mathrm{P_1} = \\mathrm{K}[\\mathrm{I} | 0]$ and $\\mathrm{P_2} = \\mathrm{K}[\\mathrm{R} | \\mathrm{t}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = make_homogeneous_pose(R,t)\n",
    "P1 = np.concatenate((K,np.array([[0],[0],[0]])),axis=1)\n",
    "P2 = K @ T[:-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then visualize the estimated relative transformation by plotting the camera matrices as simple 3D frustrums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_frustrum = np.array([[-1,-1,1,1],\n",
    "               [1,-1,1,1],\n",
    "               [1,1,1,1],\n",
    "               [-1,1,1,1],\n",
    "               [-1,-1,1,1],\n",
    "               [0,0,0,1],\n",
    "               [1,-1,1,1],\n",
    "               [1,1,1,1],\n",
    "               [0,0,0,1],\n",
    "               [-1,1,1,1]])\n",
    "\n",
    "cam_frustrum1 = cam_frustrum\n",
    "\n",
    "cam_frustrum2 = (T @ cam_frustrum.transpose()).transpose()\n",
    "\n",
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('$X$')\n",
    "ax.set_ylabel('$Y$')\n",
    "ax.set_zlabel('$Z$')\n",
    "\n",
    "ax.plot(cam_frustrum1[:,0], cam_frustrum1[:,1], cam_frustrum1[:,2])\n",
    "ax.plot(cam_frustrum2[:,0], cam_frustrum2[:,1], cam_frustrum2[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulation\n",
    "\n",
    "Once we have matching image points, and the estimated relative transformation between the two views, we can triangulate the corresponding 3D points. This can be done geometrically, by simply considering the rays corresponding to each matching image point and finding their intersection in 3D.\n",
    "\n",
    "Or, we can do this algebraically which is easier to implement.\n",
    "\n",
    "Consider two matching image points $x_1$ and $x_2$ which correspond to the same 3D point $\\mathrm{X}$ in the scene. We know $x_1 = \\mathrm{P_1}\\mathrm{X}$ and $x_2 = \\mathrm{P_2}\\mathrm{X}$. From these two equations we can form a homogeneous sytem of the form $\\mathrm{A}\\mathrm{X} = 0$ where $$A = \\begin{pmatrix}u_1p_1^{3T} - p_1^{1T}\\\\v_1p_1^{3T} - p_1^{2T}\\\\u_2p_2^{3T} - p_2^{1T}\\\\v_2p_2^{3T} - p_2^{2T}\\end{pmatrix}$$ $P_1^{1T}$ is the first row of $\\mathrm{P1}$, $(u_1, v_1)$ are the image coordinates in the first image and so on. Recall that we can solve for this vector $\\mathrm{X}$ by finding the singular vector of $\\mathrm{A}$ that corresponds to the smallest singular value. This gives us the 3D point $\\mathrm{X}$.\n",
    "\n",
    "We can then repeat this for all image points and obtain a full sparse 3D point cloud as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triangulate_points(P1, P2, pts1, pts2):\n",
    "    \"\"\" Estimates the triangulated 3D points from matching image points and\n",
    "    the corresponding camera matrices using algebraic triangulation.\n",
    "    \n",
    "    Args:\n",
    "        P1: 3x4 first camera matrix.\n",
    "        P2: 3x4 second camera matrix.\n",
    "        pts1: nx2 image points from first image.\n",
    "        pts2: nx2 matching image points from second image.\n",
    "        \n",
    "    Returns:\n",
    "        pts_3d: Triangulated 3D points\n",
    "        \n",
    "    \"\"\"\n",
    "    print('Write me!')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_3d = triangulate_points(P1, P2, pts1, pts2)\n",
    "\n",
    "################ OR #################\n",
    "\n",
    "#pts_3d = cv2.triangulatePoints(P1, P2, pts1.transpose(), pts2.transpose())\n",
    "#pts_3d = pts_3d/pts_3d[-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim3d(-5,5)\n",
    "ax.set_ylim3d(-5,5)\n",
    "ax.set_zlim3d(-5,5)\n",
    "ax.set_xlabel('$X$')\n",
    "ax.set_ylabel('$Y$')\n",
    "ax.set_zlabel('$Z$')\n",
    "\n",
    "ax.scatter(pts_3d[0,:],pts_3d[1,:],pts_3d[2,:])\n",
    "ax.plot(cam_frustrum1[:,0], cam_frustrum1[:,1], cam_frustrum1[:,2])\n",
    "ax.plot(cam_frustrum2[:,0], cam_frustrum2[:,1], cam_frustrum2[:,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
