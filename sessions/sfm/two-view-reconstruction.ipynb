{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-view reconstruction\n",
    "\n",
    "A toy exercise to reconstruct the sparse 3D geometry of a scene from two arbitrary (non-stereo) images.\n",
    "\n",
    "Concepts used:\n",
    "- Feature extraction and matching\n",
    "- Essential matrix\n",
    "- Linear triangulation\n",
    "\n",
    "@author: Karnik (karnikram@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    \"\"\" Loads image from the given path and returns\n",
    "    it in grayscale format.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path)\n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        print(path + ' could not be loaded!')\n",
    "        \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(img):\n",
    "    \"\"\" Extracts keypoints from the image and computes their descriptors.\n",
    "    \n",
    "    We use OpenCV's implementation of SIFT to do this.\n",
    "    \n",
    "    Args:\n",
    "        img: The input grayscale image.\n",
    "    \n",
    "    Returns:\n",
    "        kp: The detected keypoints.\n",
    "        ds: The feature descriptors needed  for matching.      \n",
    "    \"\"\"\n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures = 2000)\n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "    \n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(kp1, kp2, des1, des2):\n",
    "    \"\"\" Matches keypoints from two images using their descriptors.\n",
    "    \n",
    "    We use OpenCV's implementation of brute-force matching to do this.\n",
    "    \n",
    "    Args:\n",
    "        kp1: Detected keypoints from the first image.\n",
    "        kp2: Detected keypoints from the second image.\n",
    "        des1: Descriptors of the keypoints from the first image.\n",
    "        des2: Descriptors of the keypoints from the second image.\n",
    "        \n",
    "    Returns:\n",
    "        matches: List of DMatch objects needed for visualization.\n",
    "        pts1: The 2d image coordinates of matched keypoints from first image.\n",
    "        pts2: The 2d image coordinates of matched keypoints from second image.\n",
    "    \"\"\"\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    \n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "\n",
    "    for match in matches:\n",
    "        pts1.append(kp1[match.queryIdx].pt)\n",
    "        pts2.append(kp2[match.trainIdx].pt)\n",
    "        \n",
    "    pts1 = np.array(pts1, dtype=np.float32)\n",
    "    pts2 = np.array(pts2, dtype=np.float32)\n",
    "    \n",
    "    return matches, pts1, pts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEssentialMat(K, pts1, pts2):\n",
    "    \n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, threshold=1)\n",
    "\n",
    "    mask = mask[:,0]\n",
    "    m_pts1 = []\n",
    "    m_pts2 = []\n",
    "    for idx, pt in enumerate(pts1):\n",
    "        if(mask[idx]):\n",
    "            m_pts1.append(pts1[idx,:])\n",
    "            m_pts2.append(pts2[idx,:])\n",
    "\n",
    "    pts1 = np.array(m_pts1)\n",
    "    pts2 = np.array(m_pts2)\n",
    "    \n",
    "    return E, pts1, pts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoverCameraPoses(K, E, pts1, pts2):\n",
    "    _, R, t, _ = cv2.recoverPose(E, pts1, pts2, K)\n",
    "    T = constructHomogeneousPose(R,t)\n",
    "\n",
    "    P1 = np.concatenate((K,np.array([[0],[0],[0]])),axis=1)\n",
    "    P2 = K @ T[:-1,]\n",
    "    \n",
    "    return P1, P2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructHomogeneousPose(R, t):\n",
    "    \"\"\" Constructs a 4x4 homogeneous transformation matrix from a 3x3 rotation matrix\n",
    "    and a 3x1 translation vector.\n",
    "    \"\"\"\n",
    "    return np.concatenate((np.concatenate((R,t),axis=1),np.array([[0,0,0,1]])),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "We've provided two images of a scene that we want to reconstruct as input.\n",
    "These two images, however, need to be from a calibrated camera i.e. a camera whose internal parameters have already been estimated. (**Q**: What are these internal parameters?)\n",
    "\n",
    "Recall that to estimate these parameters we use multiple images of a known pattern. Images of one such pattern taken from the same camera have been provided in `./calib-images/`. Run the provided `calib.py` script on these images to estimate this camera's internal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TODO #########\n",
    "K = np.array([[?, ?, ?],\n",
    "              [?, ?, ?],\n",
    "              [?, ?, ?]])\n",
    "######################\n",
    "\n",
    "#K = np.array([[551.7050819120993, 0.0, 284.0215789988593],\n",
    "#               [0.0, 551.5259346746498, 186.04513199963452],\n",
    "#               [0.0, 0.0, 1.0]])\n",
    "\n",
    "img1 = load_image('./img1.png')\n",
    "img2 = load_image('./img2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "\n",
    "Next, we identify a few salient points (or 'keypoints') from the images that will help us to estimate the camera motion between the two images. Here, we extract these keypoints using the `extract_features` function and then we visualize them. Notice that almost no keypoints belong to the table surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, des1 = extract_features(img1)\n",
    "kp2, des2 = extract_features(img2)\n",
    "\n",
    "fix, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(img1, cmap='gray')\n",
    "ax[1].imshow(img2, cmap='gray')\n",
    "\n",
    "for idx in range(len(kp1)):\n",
    "    ax[0].plot(kp1[idx].pt[0], kp1[idx].pt[1], 'r*')\n",
    "    \n",
    "for idx in range(len(kp2)):\n",
    "    ax[1].plot(kp2[idx].pt[0], kp2[idx].pt[1], 'b*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature matching\n",
    "\n",
    "We now wish to match these extracted keypoints from both the images. That is, we want to identify keypoints from both the images that correspond to the same 3D point in the scene. Here, we do this matching using the `match_features` function and then we visualize the matches. Notice that there are some incorrect matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches, pts1, pts2 = match_features(kp1, kp2, des1, des2)\n",
    "draw_params = dict(matchColor = (0,255,0), singlePointColor=(255,0,0), flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, **draw_params)\n",
    "\n",
    "plt.imshow(img3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating camera motion\n",
    "\n",
    "Next, we estimate the camera motion between the two frames by estimating the corresponding essential matrix. Recall that the essential matrix is a 3x3 matrix that encodes the relative orientation between the two views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, pts1, pts2 = findEssentialMat(K, pts1, pts2)\n",
    "P1, P2 = recoverCameraPoses(K, E, pts1, pts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_3d = cv2.triangulatePoints(P1, P2, pts1.transpose(), pts2.transpose())\n",
    "pts_3d = pts_3d/pts_3d[-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim3d(-10,10)\n",
    "ax.set_ylim3d(-10,10)\n",
    "ax.set_zlim3d(-10,10)\n",
    "ax.scatter(pts_3d[0,:],pts_3d[1,:],pts_3d[2,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
